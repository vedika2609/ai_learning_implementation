{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 1. Data preparation for getting started with LLM Ops\n",
    "\n",
    "Below are cells to instantiate the vertex ai and bigquery client.\n",
    "Big query client is then heavily used to extract data from the stack exchange DB and bifurcate the data in evaluation and model tuning based on user's inputs"
   ],
   "id": "8a77807f92f26ee"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "from utils.utils_gcp import get_bq_client",
   "id": "8eaf8991efc8e256",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "bq_client = get_bq_client()\n",
    "print(\"Big query client available\")"
   ],
   "id": "310b115feecd14eb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "QUERY_TABLES = \"\"\"\n",
    "SELECT\n",
    "  table_name\n",
    "FROM\n",
    "  `bigquery-public-data.stackoverflow.INFORMATION_SCHEMA.TABLES`\n",
    "\"\"\""
   ],
   "id": "3622b37bba561a5d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "query_job = bq_client.query(QUERY_TABLES)",
   "id": "669a1f932dea11d2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for row in query_job:\n",
    "    for value in row.values():\n",
    "        print(value)"
   ],
   "id": "962bfca7f42ea308",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "INSPECT_QUERY = \"\"\"\n",
    "SELECT\n",
    "    *\n",
    "FROM\n",
    "    `bigquery-public-data.stackoverflow.posts_questions`\n",
    "LIMIT 3\n",
    "\"\"\""
   ],
   "id": "5b172d4393b4e947",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import pyarrow as pa"
   ],
   "id": "ce3f30fbd28e7904",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "query_job  = bq_client.query(INSPECT_QUERY)",
   "id": "bfe118fbfaf70a49",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "try:\n",
    "    stack_overflow_df = query_job\\\n",
    "    .result()\\\n",
    "    .to_arrow()\\\n",
    "    .to_pandas()\n",
    "    print(stack_overflow_df.head())\n",
    "\n",
    "except Exception as e:\n",
    "    print('Exception occurred.', e)"
   ],
   "id": "58ee5165914eab4c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "QUERY_ALL = \"\"\"\n",
    "SELECT\n",
    "    *\n",
    "FROM\n",
    "    `bigquery-public-data.stackoverflow.posts_questions` q\n",
    "\"\"\""
   ],
   "id": "742dce99cca63898",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "query_job = bq_client.query(QUERY_ALL)",
   "id": "5e0c362eece4796d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "try:\n",
    "    stack_overflow_df = query_job\\\n",
    "    .result()\\\n",
    "    .to_arrow()\\\n",
    "    .to_pandas()\n",
    "except Exception as e:\n",
    "    print('The DataFrame is too large to load into memory.', e)"
   ],
   "id": "f947513c1198c565",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "QUERY = \"\"\"\n",
    "SELECT\n",
    "    CONCAT(q.title, q.body) as input_text,\n",
    "    a.body AS output_text\n",
    "FROM\n",
    "    `bigquery-public-data.stackoverflow.posts_questions` q\n",
    "JOIN\n",
    "    `bigquery-public-data.stackoverflow.posts_answers` a\n",
    "ON\n",
    "    q.accepted_answer_id = a.id\n",
    "WHERE\n",
    "    q.accepted_answer_id IS NOT NULL AND\n",
    "    REGEXP_CONTAINS(q.tags, \"python\") AND\n",
    "    a.creation_date >= \"2020-01-01\"\n",
    "LIMIT\n",
    "    10000\n",
    "\"\"\""
   ],
   "id": "7acd3728b3f09570",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "query_job = bq_client.query(QUERY)",
   "id": "644434904ff54a37",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "stack_overflow_df = query_job .result().to_arrow().to_pandas()\n",
    "print(stack_overflow_df.head())"
   ],
   "id": "d60ee29d69f8c636",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "INSTRUCTION_TEMPLATE = f\"\"\"\\\n",
    "Please answer the following Stackoverflow question on Python. Answer it like you are a developer answering Stackoverflow questions.\n",
    "Stackoverflow question:\n",
    "\"\"\""
   ],
   "id": "5914721d0c05babb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "stack_overflow_df['input_text_instruct'] = INSTRUCTION_TEMPLATE + ' '\\\n",
    "    + stack_overflow_df['input_text']"
   ],
   "id": "5df86ecb89a8a39a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "from sklearn.model_selection import train_test_split",
   "id": "a718e9d14124fe2b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Divide the data into a training and evaluation. By default, 80/20 split is used.\n",
    "# This (80/20 split) allows for more data to be used for tuning. The evaluation split is used as unseen data during tuning to evaluate performance.\n",
    "# The random_state parameter is used to ensure random sampling for a fair comparison.\n",
    "# test_size=0.2 means 20% for evaluation which then makes train set to be of 80%\n",
    "\n",
    "\n",
    "train, evaluation = train_test_split(\n",
    "    stack_overflow_df,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")"
   ],
   "id": "64aac25a801747a5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "import datetime",
   "id": "4a59a2fe7ff5fac5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "date = datetime.datetime.now().strftime(\"%H:%d:%m:%Y\")",
   "id": "35498e3176d3d488",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "cols = ['input_text_instruct','output_text']\n",
    "tune_jsonl = train[cols].to_json(orient=\"records\", lines=True)\n",
    "evaluation_jsonl = evaluation[cols].to_json(orient=\"records\", lines=True)"
   ],
   "id": "bf0d6ab6ad21e0d0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "training_data_filename = f\"tune_data_stack_overflow_python_qa.jsonl\"\n",
    "evaluation_data_filename = f\"tune_eval_data_stack_overflow_python_qa.jsonl\""
   ],
   "id": "f54087b7f9c9d950",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with open(training_data_filename, \"w\") as f:\n",
    "    f.write(tune_jsonl)\n",
    "\n",
    "with open(evaluation_data_filename, \"w\") as f:\n",
    "    f.write(evaluation_jsonl)"
   ],
   "id": "d9ddbfcdb5dcf4d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 2. Automation and orchestration with pipelines\n",
    "\n",
    "Below are cells to showcase orchestration and automation of a workflow using [Kubeflow Pipelines](https://www.kubeflow.org/docs/components/pipelines/v2/)\n",
    "\n",
    "Kubeflow Pipelines is an open source framework.\n",
    "It's like a construction kit for building machine learning pipelines, making it easy to orchestrate and automate complex tasks."
   ],
   "id": "2e2de15169103b4c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "import kfp\n",
    "from kfp import dsl\n",
    "from kfp import compiler\n",
    "from kfp.local import SubprocessRunner\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",\n",
    "                        category=FutureWarning,\n",
    "                        module='kfp.*')\n",
    "\n",
    "kfp.local.init(runner=SubprocessRunner())"
   ],
   "id": "7e333cb772cef8d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "@dsl.component\n",
    "def say_hello(name: str) -> str:\n",
    "    hello_message = f\"Hello, {name}!\"\n",
    "    return hello_message"
   ],
   "id": "519a7aed6c5c131",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- Note when passing in values to the a `dsl.component` function, you have to specify the argument names (keyword arguments), and can't use positional arguments.",
   "id": "ea0ab9d2d7228302"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "hello_task = say_hello(name=\"vedika\")\n",
    "print(hello_task)"
   ],
   "id": "74479ad975bd5a3a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "@dsl.component\n",
    "def how_are_you(hello_message: str) -> str:\n",
    "    how_are_you_message = f\"How are you? {hello_message}\"\n",
    "    return how_are_you_message"
   ],
   "id": "7e4be5006bc64985",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- Notice that when we pass in the return value from the `say_hello` function, we want to pass in the PipelineTask.output object, and not the PipelineTask object itself.",
   "id": "c46202dfbaf12ff7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "@dsl.pipeline\n",
    "def hello_pipeline(recipient: str) -> str:\n",
    "    hello_task = say_hello(name=recipient)\n",
    "    how_are_you_task = how_are_you(hello_message=hello_task.output)\n",
    "    return how_are_you_task.output"
   ],
   "id": "3beef5a75b73f56a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Implement the pipeline\n",
    "\n",
    "- A pipeline is a set of components that you orchestrate.\n",
    "- It lets you define the order of execution and how data flows from one step to another.\n",
    "- Compile the pipeline into a yaml file, `pipeline.yaml`\n",
    "- You can look at the `pipeline.yaml` file in your workspace by going to `File --> Open...`. Or right here in the notebook (two cells below)"
   ],
   "id": "3a231c095fce88d9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "compiler.Compiler().compile(hello_pipeline, 'hello_pipeline.yaml')",
   "id": "9be8c3c222c1d061",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "pipeline_arguments = {\n",
    "    \"recipient\": \"World!\",\n",
    "}"
   ],
   "id": "8ada1ebc7a774eae",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!cat hello_pipeline.yaml",
   "id": "c1fc81fb88c0ed88",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from google.cloud.aiplatform import PipelineJob\n",
    "from utils.utils_gcp import init_vertex_ai\n",
    "\n",
    "init_vertex_ai()\n",
    "job = PipelineJob(\n",
    "        ### path of the yaml file to execute\n",
    "        template_path=\"hello_pipeline.yaml\",\n",
    "        ### name of the pipeline\n",
    "        display_name=f\"deep_learning_ai_pipeline\",\n",
    "        ### pipeline arguments (inputs)\n",
    "        ### {\"recipient\": \"World!\"} for this example\n",
    "        parameter_values=pipeline_arguments,\n",
    "        ### region of execution\n",
    "        location=\"us-central1\",\n",
    "        ### root is where temporary files are being\n",
    "        ### stored by the execution engine\n",
    "        pipeline_root=\"./\",\n",
    ")"
   ],
   "id": "2d34d497fc5f17",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "job.submit() #submits job for execution",
   "id": "cf0ad0474a320658",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "job.state #check to see the status of the job",
   "id": "d35366d381f94960",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "TRAINING_DATA_URI = \"./tune_data_stack_overflow_python_qa.jsonl\"\n",
    "EVALUATION_DATA_URI = \"./tune_eval_data_stack_overflow_python_qa.jsonl\""
   ],
   "id": "f9b6e6dbd36183f3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- Provide the model with a version.\n",
    "- Versioning model allows for:\n",
    "  - Reproducibility: Reproduce your results and ensure your models perform as expected.\n",
    "  - Auditing: Track changes to your models.\n",
    "  - Rollbacks: Roll back to a previous version of your model."
   ],
   "id": "74082f05e6f9808d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "template_path = 'https://us-kfp.pkg.dev/ml-pipeline/large-language-model-pipelines/tune-large-model/v2.0.0'\n",
    "date = datetime.datetime.now().strftime(\"%H:%d:%m:%Y\")\n",
    "MODEL_NAME = f\"deep-learning-ai-model-{date}\""
   ],
   "id": "27e867732ad607ec",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- This example uses two PaLM model parameters:\n",
    "  - `TRAINING_STEPS`: Number of training steps to use when tuning the model. For extractive QA you can set it from 100-500.\n",
    "  - `EVALUATION_INTERVAL`: The interval determines how frequently a trained model is evaluated against the created *evaluation set* to assess its performance and identify issues. Default will be 20, which means after every 20 training steps, the model is evaluated on the evaluation dataset."
   ],
   "id": "e9d4afb8650638f5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "TRAINING_STEPS = 200\n",
    "EVALUATION_INTERVAL = 20"
   ],
   "id": "94d4070c2e55cc8b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "pipeline_arguments = {\n",
    "    \"model_display_name\": MODEL_NAME,\n",
    "    \"location\":  \"us-central1\",\n",
    "    \"large_model_reference\": \"text-bison@001\",\n",
    "    \"project\": \"light-river-469808-p4\",\n",
    "    \"train_steps\": TRAINING_STEPS,\n",
    "    \"dataset_uri\": TRAINING_DATA_URI,\n",
    "    \"evaluation_interval\": EVALUATION_INTERVAL,\n",
    "    \"evaluation_data_uri\": EVALUATION_DATA_URI,\n",
    "}\n",
    "\n",
    "pipeline_root = \"./\"\n",
    "\n",
    "job = PipelineJob(template_path=template_path,\n",
    "                  display_name=f\"deep_learning_ai_pipeline-{date}\",\n",
    "                    parameter_values=pipeline_arguments,\n",
    "                  location=\"us-central1\",\n",
    "                  pipeline_root= pipeline_root,\n",
    "                  enable_caching= True)"
   ],
   "id": "f9f6c35843bb95b9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "job.submit()",
   "id": "dc27e40559bef471",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# L4: Predictions, Prompts and Safety",
   "id": "8f682692ebc6d774"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import vertexai\n",
    "from vertexai.language_models import TextGenerationModel\n",
    "from utils.utils_gcp import init_vertex_ai"
   ],
   "id": "7ec0e8da12945b61",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "init_vertex_ai()",
   "id": "5cd17410dce3b849",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = TextGenerationModel.from_pretrained(\"text-bison@001\")\n",
    "list_tuned_models = model.list_tuned_model_names()\n",
    "for i in list_tuned_models:\n",
    "    print (i)"
   ],
   "id": "9a8af817a8a6eb81",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import random\n",
    "tuned_model_select = random.choice(list_tuned_models)\n",
    "\n",
    "deployed_model = TextGenerationModel.get_tuned_model(tuned_model_select)\n",
    "PROMPT = \"How can I load a csv file using Pandas?\"\n",
    "response = deployed_model.predict(PROMPT)\n",
    "print(response)"
   ],
   "id": "ea3e624d28f06194",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "from pprint import pprint",
   "id": "bb7e68c0bf7f1f5f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "output = response._prediction_response[0]\n",
    "pprint(output)\n",
    "final_output = response._prediction_response[0][0][\"content\"]\n",
    "print(final_output)"
   ],
   "id": "a41b6a1019b2b6d5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Prompt Management and Templates\n",
    "- Remember that the model was trained on data that had an `Instruction` and a `Question` as a `Prompt` (Lesson 2).\n",
    "- In the example above, *only*  a `Question` as a `Prompt` was used for a response.\n",
    "- It is important for the production data to be the same as the training data. Difference in data can effect the model performance.\n",
    "- Add the same `Instruction` as it was used for training data, and combine it with a `Question` to be used as a `Prompt`."
   ],
   "id": "d53681bd923e9501"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "INSTRUCTION = \"\"\"\\\n",
    "Please answer the following Stackoverflow question on Python.\\\n",
    "Answer it like\\\n",
    "you are a developer answering Stackoverflow questions.\\\n",
    "Question:\n",
    "\"\"\"\n",
    "\n",
    "QUESTION = \"How can I store my TensorFlow checkpoint on\\\n",
    "Google Cloud Storage? Python example?\"\n",
    "\n",
    "PROMPT = f\"\"\"\n",
    "{INSTRUCTION} {QUESTION}\n",
    "\"\"\"\n",
    "print(PROMPT)"
   ],
   "id": "efd41955b17bb350",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "final_response = deployed_model.predict(PROMPT)\n",
    "output = final_response._prediction_response[0][0][\"content\"]\n",
    "print(output)"
   ],
   "id": "32d122a1cf77c0a1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Safety Attributes\n",
    "- The reponse also includes safety scores.\n",
    "- These scores can be used to make sure that the LLM's response is within the boundries of the expected behaviour.\n",
    "- The first layer for this check, `blocked`, is by the model itself."
   ],
   "id": "3d0439b16babd6e7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "### retrieve the \"blocked\" key from the\n",
    "### \"safetyAttributes\" of the response\n",
    "blocked = response._prediction_response[0][0]\\\n",
    "['safetyAttributes']['blocked']\n",
    "print(blocked)"
   ],
   "id": "52b96cfd2a7b33c2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- The second layer of this check can be defined by you, as a practitioner, according to the thresholds you set.\n",
    "- The response returns probabilities for each safety score category which can be used to design the thresholds."
   ],
   "id": "265af57ac03becf1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "### retrieve the \"safetyAttributes\" of the response\n",
    "safety_attributes = response._prediction_response[0][0]['safetyAttributes']\n",
    "pprint(safety_attributes)"
   ],
   "id": "2042e25948067456",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Citations\n",
    "- Ideally, a LLM should generate as much original cotent as possible.\n",
    "- The `citationMetadata` can be used to check and reduce the chances of a LLM generating a lot of existing content."
   ],
   "id": "8b793321a1fb51c0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "### retrieve the \"citations\" key from the\n",
    "### \"citationMetadata\" of the response\n",
    "citation = response._prediction_response[0][0]\\\n",
    "['citationMetadata']['citations']"
   ],
   "id": "a25da23c40738b4c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "pprint(citation)",
   "id": "ef70edf42f5d1ddb",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
